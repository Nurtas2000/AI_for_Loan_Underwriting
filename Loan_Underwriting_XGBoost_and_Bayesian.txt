import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
from sklearn.impute import SimpleImputer
import xgboost as xgb
import optuna
from optuna.samplers import TPESampler
from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# ====================== DATA PREPARATION ======================
data = """application_id,age,income,employment_length,credit_score,debt_to_income,loan_amount,loan_term,home_ownership,loan_purpose,existing_customer,previous_defaults,recent_inquiries,approved,repayment_history_12mo,defaulted
APP1001,35,85000,5.2,720,0.18,25000,36,Mortgage,Debt consolidation,1,0,2,1,111111111111,0
APP1002,42,62000,3.8,685,0.32,18000,24,Rent,Home improvement,0,1,5,1,111101111011,0
APP1003,28,48000,1.5,655,0.41,12000,36,Rent,Personal,0,0,3,0,NA,NA
APP1004,51,112000,10.7,745,0.22,35000,60,Own,Business,1,0,1,1,111111111111,0
APP1005,33,54000,2.3,630,0.38,15000,24,Rent,Education,0,2,4,0,NA,NA
APP1006,47,78000,7.1,710,0.27,28000,48,Mortgage,Debt consolidation,1,1,2,1,111110111101,1
APP1007,31,45000,4.0,590,0.45,10000,12,Rent,Personal,0,0,6,0,NA,NA
APP1008,39,92000,6.5,695,0.29,22000,36,Mortgage,Home improvement,1,0,3,1,111111111111,0
APP1009,24,38000,0.8,610,0.52,8000,24,Rent,Education,0,0,7,0,NA,NA
APP1010,56,105000,12.3,760,0.19,40000,60,Own,Business,1,0,0,1,111111111111,0
APP1011,29,51000,3.2,640,0.36,14000,36,Rent,Personal,0,1,4,1,111011101110,1
APP1012,44,67000,5.9,675,0.31,20000,24,Rent,Debt consolidation,0,0,3,1,111111111111,0
APP1013,37,58000,4.7,625,0.42,13000,12,Rent,Education,0,2,5,0,NA,NA
APP1014,50,95000,8.6,730,0.24,30000,48,Mortgage,Home improvement,1,0,1,1,111111111111,0
APP1015,27,42000,2.1,605,0.47,9000,24,Rent,Personal,0,0,8,0,NA,NA
APP1016,40,83000,6.8,700,0.26,25000,36,Mortgage,Debt consolidation,1,1,2,1,111101111011,0
APP1017,32,49000,3.5,615,0.43,11000,12,Rent,Education,0,0,6,0,NA,NA
APP1018,48,88000,9.2,715,0.23,32000,60,Own,Business,1,0,2,1,111111111111,0
APP1019,23,36000,1.0,585,0.55,7000,24,Rent,Personal,0,0,9,0,NA,NA
APP1020,53,98000,11.5,750,0.21,38000,48,Mortgage,Home improvement,1,0,0,1,111111111111,0
APP1021,30,47000,2.8,620,0.39,12000,24,Rent,Education,0,1,5,1,110111011101,1
APP1022,45,71000,6.3,690,0.28,22000,36,Rent,Debt consolidation,0,0,3,1,111111111111,0
APP1023,38,63000,5.0,650,0.35,17000,24,Rent,Personal,0,2,4,1,111011101110,0
APP1024,52,102000,10.1,740,0.20,35000,60,Own,Business,1,0,1,1,111111111111,0
APP1025,26,41000,1.7,600,0.48,9000,12,Rent,Education,0,0,7,0,NA,NA
APP1026,41,76000,7.4,705,0.25,27000,48,Mortgage,Debt consolidation,1,1,2,1,111101111011,0
APP1027,34,52000,3.9,635,0.37,15000,24,Rent,Personal,0,0,5,1,111111111111,0
APP1028,49,91000,8.9,725,0.22,33000,60,Own,Home improvement,1,0,1,1,111111111111,0
APP1029,22,34000,0.5,570,0.58,6000,12,Rent,Education,0,0,10,0,NA,NA
APP1030,54,110000,12.0,755,0.18,42000,48,Mortgage,Business,1,0,0,1,111111111111,0
APP1031,25,39000,1.3,595,0.50,8000,24,Rent,Personal,0,1,8,0,NA,NA
APP1032,46,69000,6.7,680,0.30,21000,36,Rent,Debt consolidation,0,0,3,1,111111111111,0
APP1033,36,57000,4.5,645,0.34,16000,24,Rent,Education,0,2,4,1,110110111011,1
APP1034,43,80000,7.7,695,0.27,29000,48,Mortgage,Home improvement,1,0,2,1,111111111111,0
APP1035,31,50000,2.9,630,0.40,13000,12,Rent,Personal,0,0,6,0,NA,NA
APP1036,55,115000,13.2,765,0.17,45000,60,Own,Business,1,0,0,1,111111111111,0
APP1037,27,44000,1.9,610,0.46,10000,24,Rent,Education,0,0,7,0,NA,NA
APP1038,47,85000,8.1,710,0.24,31000,36,Mortgage,Debt consolidation,1,1,1,1,111101111011,0
APP1039,20,32000,0.2,560,0.62,5000,12,Rent,Personal,0,0,11,0,NA,NA
APP1040,57,120000,14.0,770,0.15,50000,60,Own,Business,1,0,0,1,111111111111,0
APP1041,29,46000,2.5,625,0.42,11000,24,Rent,Education,0,1,5,1,111011101110,0
APP1042,52,99000,10.5,735,0.21,36000,48,Mortgage,Home improvement,1,0,1,1,111111111111,0
APP1043,35,55000,4.2,640,0.33,18000,24,Rent,Personal,0,2,4,1,110111011101,1
APP1044,40,74000,7.0,700,0.26,26000,36,Rent,Debt consolidation,0,0,3,1,111111111111,0
APP1045,58,125000,15.0,775,0.14,55000,60,Own,Business,1,0,0,1,111111111111,0
APP1046,24,37000,1.1,590,0.53,7500,12,Rent,Education,0,0,9,0,NA,NA
APP1047,48,87000,9.0,720,0.23,34000,48,Mortgage,Home improvement,1,1,2,1,111101111011,0
APP1048,33,53000,3.7,630,0.38,17000,24,Rent,Personal,0,0,5,1,111111111111,0
APP1049,59,130000,16.0,780,0.13,60000,60,Own,Business,1,0,0,1,111111111111,0
APP1050,21,33000,0.3,565,0.60,5500,12,Rent,Education,0,0,12,0,NA,NA"""

df = pd.read_csv(pd.compat.StringIO(data))

# Feature Engineering
def process_repayment_history(row):
    if pd.isna(row['repayment_history_12mo']):
        return np.nan
    return str(row['repayment_history_12mo']).count('0') / len(str(row['repayment_history_12mo']))

df['missed_payments_ratio'] = df.apply(process_repayment_history, axis=1)
df['income_to_loan_ratio'] = df['income'] / df['loan_amount']
df['age_group'] = pd.cut(df['age'], bins=[18, 25, 35, 45, 55, 65], labels=['18-25', '26-35', '36-45', '46-55', '56-65'])

# Prepare data for modeling
approved_df = df[df['approved'] == 1].copy()
rejected_df = df[df['approved'] == 0].copy()

# Features and target
features = ['age', 'income', 'employment_length', 'credit_score', 'debt_to_income', 
            'loan_amount', 'loan_term', 'home_ownership', 'loan_purpose', 
            'existing_customer', 'previous_defaults', 'recent_inquiries',
            'missed_payments_ratio', 'income_to_loan_ratio']
target = 'defaulted'

# Split into train/test
X = approved_df[features]
y = approved_df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Preprocessing pipeline
numeric_features = ['age', 'income', 'employment_length', 'credit_score', 'debt_to_income',
                   'loan_amount', 'loan_term', 'recent_inquiries', 'missed_payments_ratio',
                   'income_to_loan_ratio']
categorical_features = ['home_ownership', 'loan_purpose']

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# ====================== BAYESIAN OPTIMIZATION ======================
def objective(trial):
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),
        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),
        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'eta': trial.suggest_float('eta', 0.01, 0.3),
        'gamma': trial.suggest_float('gamma', 0, 1),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 10)
    }
    
    model = xgb.XGBClassifier(**params)
    
    # Preprocess and fit
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                              ('classifier', model)])
    pipeline.fit(X_train, y_train)
    
    # Evaluate
    y_pred = pipeline.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_pred)
    
    return auc

study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))
study.optimize(objective, n_trials=50, timeout=600)

# ====================== MODEL TRAINING ======================
best_params = study.best_params
best_params.update({'objective': 'binary:logistic', 'eval_metric': 'auc'})

final_model = xgb.XGBClassifier(**best_params)
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                          ('classifier', final_model)])
pipeline.fit(X_train, y_train)

# ====================== REJECT INFERENCE ======================
# Step 1: Predict probabilities for rejected applications
rejected_X = rejected_df[features]
rejected_probs = pipeline.predict_proba(rejected_X)[:, 1]

# Step 2: Assign inferred outcomes (various techniques possible)
# Here we use a simple threshold approach
threshold = 0.5
rejected_df['inferred_default'] = (rejected_probs >= threshold).astype(int)

# Step 3: Combine with original data
augmented_df = pd.concat([approved_df, rejected_df], axis=0)

# Step 4: Retrain model on augmented data (optional)
# X_augmented = augmented_df[features]
# y_augmented = augmented_df['defaulted'].fillna(augmented_df['inferred_default'])
# pipeline.fit(X_augmented, y_augmented)

# ====================== EVALUATION ======================
# Performance metrics
y_pred = pipeline.predict_proba(X_test)[:, 1]
y_pred_class = (y_pred >= 0.5).astype(int)

print("\nClassification Report:")
print(classification_report(y_test, y_pred_class))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_class))

print(f"\nROC AUC Score: {roc_auc_score(y_test, y_pred):.4f}")

# Fairness metrics
X_test_preprocessed = preprocessor.transform(X_test)
X_test_df = pd.DataFrame(X_test_preprocessed, columns=(
    numeric_features + 
    list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features))
)

print("\nFairness Metrics:")
print(f"Demographic Parity Difference: {demographic_parity_difference(y_test, y_pred_class, sensitive_features=X_test['age_group']):.4f}")
print(f"Equalized Odds Difference: {equalized_odds_difference(y_test, y_pred_class, sensitive_features=X_test['age_group']):.4f}")

# Feature Importance
feature_importances = final_model.feature_importances_
feature_names = numeric_features + list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features))
importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances}).sort_values('importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='importance', y='feature', data=importance_df.head(15))
plt.title('Top 15 Feature Importances')
plt.tight_layout()
plt.show()

# ====================== DEPLOYMENT PREP ======================
import joblib
joblib.dump(pipeline, 'loan_underwriting_model.pkl')
joblib.dump(preprocessor, 'loan_preprocessor.pkl')

print("\nModel and preprocessor saved for deployment!")
